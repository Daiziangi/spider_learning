# 爬虫

### 	常用库:

 -  selenium  ->可爬动态数据,需要下载与自己谷歌浏览器对应的ChromeDriver,并放置在对应脚本(项目)的目录下.

    #### 和页面中的元素进行交互(selenium)

    1. 找到元素
        - element = driver.find_element()
        - element = driver.find_elements()

    2. 声明我们的操作
        - element.click()  点击元素
        - element.send_keys("输入具体的内容")  输入内容
        - driver.page_source  获取页面中的内容
       ...

    #### Chromedriver 路径一般放在两个地方:

    - 一个是系统变量中
    - 另一个就是当前会调用到Chromedriver的文件所在的目录(比如现在这个情况,驱动内容就在同名目录下)

 -  bs4->爬静态,bs4中的BeautifulSoup实例化后可以用来解析网页的静态资源树

 -  lxml->爬静态,其中etree,html也都可以解析静态资源树,并以此进行查找元素(利用xpath/css选择器...)

    #### Xpath知识点:

    1. 步骤:
        1. 构造请求头,用requests发出请求,得到response
        2. 将response中的text形式放入到lxml中的etree.HTML构造出一个(xpath)解析树<br>也可以是将response中的content放入到lxml中的html.fromstring中构造柱解析树
        3. 若为xpath解析树则调用其中的xpath方法,传入对应元素的xpath进行搜索查找;若要使用css选择的话,则要调用html.fromstring构造出的解析树的cssselect方法,向其中传入对应元素的css查找路径进行搜索查找
        4. 搜索查找后得到的列表中的各个内容仍可以继续调用xpath方法/cssselect方法进行二级查找,也可以将这个列表中的各个内容单独的拿出来进行各种处理,比如取出数据进行保存在哪些文件之类的.

    2. xpath常用函数:
        - contains(@属性选择,"属性中包含的值")   如: contains(@class,'bookname') 则选中类名包含着'bookname'这个字符串的标签
          也可以不使用contains函数,直接写 @class='bookname'  则为选中类名为 'bookname'的标签.
          这两种也常用 [] 包裹起来
        - text()   提取出当前标签中的文本  如 //*h1/text() 则为提取出所有h1标签中的文本内容

    3. xpath常用语法规则:

        - '//'<p> 表示从头开始全局找指定元素

        - './' <p> 表示从当前层次位置开始找指定元素

        - tree.xpath("按照语法规则写入具体的查找对象层次及其特征")得到返回的东西也可以继续进行 .xpath()
          若此时.xpath()时以 './'开头,则以上次xpath()到的层级开始向下查找指定元素

    4. #### css选择器的选取规则
        1. '#'后面跟着的是元素的 ID 名称，用于选择具有特定 ID 属性的元素。ID 在一个页面中应该是唯一的。    
            '#'uniqueId 会选择 ID 为 uniqueId 的元素。

        2. '.' 后面跟着的是元素的类名，用于选择所有具有特定类名的元素。    
           .className 会选择所有具有 class="className" 属性的元素。

        3. '[]' 用于选择具有特定属性的元素。你可以进一步指定属性的值来缩小选择范围。        
           [type="text"] 会选择所有 type 属性为 text 的元素。

        4. 以空格为分层次的标志,而xpath中以单斜杆作为分层次的标志.

        5.   在 Scrapy 框架或 parsel 库中，::text 是一个特殊的选择器，用于选择元素的文本内容。
             所以，如果你在 Scrapy 框架中看到 span.inq::text，这意味着你想要选择所有类名为 inq 的 span 元素的文本内容。
             这是一个特定于 Scrapy 的用法，而不是标准的 CSS 语法。

             在 CSS 中，用于选择文本内容的正确伪元素是 ::selection，它用于选择用户选中的文本部分。
             但是，它不能与 span 或任何其他标签一起使用来选择文本内容。	

- parsel -->提供css/xpath选择器

  #### 常用函数和用法

  - selector = parsel.Selector(response.text)  --->将响应的文本内容放入选择器中进行实例化
  - selector.css('css选择器需要选中的内容').get()   --->获取选中的内容中的第一个数据. --->返回字符串.
    - selector.css('css选择器需要选中的内容::text').get()   --->获取选中的内容中第一个数据的文本内容.
    - selector.css('css选择器需要选中的内容::text').getall()   --->获取所有选中的内容数据中的文本内容.  --->返回装有字符串的列表  
      - 常常与 '\n'.join()联用,将列表中的字符串以换行符相连得到一个完整的长文本.

- requests  ->不支持http2

- httpx   ->支持http2,用法也和requests差不多

- scrapy  ->是个框架,有助于提升开发效率.

  常用命令:

  - scrapy startproject 项目名  -->创建项目
  - scrapy genspider 爬虫名 爬虫要爬的源域名  -->在项目文件夹下执行,自动生成配置对应的爬虫源文件.
  - scrapy crawl 对应要启动的爬虫的名称 
    - -o 爬取到的数据要存放的文件名(支持csv,json,xml)
    - --nolog   --->不显示日志信息.

  注意点:

  - 在scrapy项目的虚拟环境中要再次 pip install scrapy 一下,再次安装一次.

    第一次安装是为了能够使用 scrapy startproject开启项目, 第二次安装是为了在项目的虚拟环境中使用scrapy 中对应需要依赖的库和包.

  - 一般需要再setting.py文件中修改USER_AGENT以伪装爬虫

  - 在中间键文件的下载中间件的process_request()方法里去拦截请求以加request.cookies,也可以用request.meta去加代理.

    并且中间键设置后要在setting文件中将对应的中间键打开.  发送请求时,后面数值小的先执行,数值大的后执行,接收请求的时候刚好相反.

  - 

- re  ->正则表达式.

  #### 常用函数

  - re.findall('需要找到的数据,中间可以用括号包裹以加匹配格式',数据来源)

  #### 匹配规则

  - .*? 表示匹配任意字符数据,除了 \n 的换行符以外. 
  
    使用实(示)例:
  
    

#### 

#### 可能遇到的问题:

- 用 scrapy crawl  spidername -o data.csv 得到的输出于"data.csv"的文件,在编译器内中文可能可以被成功解析,而在外部用Excel打开则可能显示为乱码.

  ###### 解决方法:

  - 暂无

- 

#### 总结出的小技巧:

- 若需要针对实现"在某些元素消失之后"再进行的操作,比如可能会使用

  ```
  wait.until(EC.invisibility_of_element_located(loading_element))
  ```

  这段代码来实现.

  而对于那些原本就不在页面中的元素,可能在元素出现之前就进行了检测,检测结果直接就为"元素已消失",因此直接往下执行代码,而出现"不等待"的情况.针对这种情况,可以用逆向思维:
  先等待元素出现,然后当元素出现后点击关闭按钮将其关闭. 

  如: 等待对话框中的关闭按钮出现并点击它

  ```
  close_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".flbc")))
  ```

- 针对一个需要获取的数据本身,它在其中一个页面里较难获取/获取起来较为麻烦,但同时其在另一个原本就需要进行爬取的网页(比如说需要进一步进入更深一层的网页中去进行爬取时——类似于简介页与详情页的关系)中也存在的话,此时去查看是否会更易获取这个相同的数据元素,若是更容易获取的话,则在"详情页"中去获取.

  如果不追求很高的效率的话,这种方法可以被视为"绕路",即使不需要在"详情页"中去爬取某些数据,但此时针对在"简介页"中较难获取的这个数据,也可以通过请求"详情页"之后再"详情页"中获取这个数据元素.





### 反爬技巧:



### 逆向: